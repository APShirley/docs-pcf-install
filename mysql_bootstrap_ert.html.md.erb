---
title: Recovering MySQL from Elastic Runtime downtime
---

<strong><%= modified_date %></strong>

This topic describes the manual process necessary to recover an Elastic Runtime cluster that has been shut off. There are times when all of the Elastic Runtime VMs have been removed, such as Terminating all VMs on AWS when Elastic Runtime is not in use. Now that PCF has HA MySQL as its internal database, it is no longer possible to have BOSH restart all of the VMs. In the event that all of the MySQL VMs are gone, the cluster will not automatically restart.


## <a id='export'>Prerequisites </a> ##

1. the key file from Ops Manager Director > Settings > SSH Private Key
1. the password for Director in Ops Manager Director > Credentials > Director
1. the IP address for the "Ops Manager Director" which is under the Status tab

## <a id='export'>Part A: Recovering MySQL </a> ##

1. Copy and paste the private key to a file. 
  ```cat > director.key; chmod 600 director.key```

1. `ssh-add director.key`**

1. `ssh -A pcf.SYSTEM-DOMAIN -l ubuntu`

1. Configure BOSH to talk to the Ops Manager Director, using the IP, login, and password obtained in the Prerequisites above.

    ```
    $ bosh target 10.0.16.5
    Target set to `p-bosh-30c19bdd43c55c627d70'
    Your username: director
    Enter password:
    Logged in as `director'
    ```
    
1. Run `bosh deployments` to discover the name of the CF job.

    ```
    $ bosh deployments
    Acting as user 'director' on 'p-bosh-30c19bdd43c55c627d70'

    +-------------------------+-------------------------------+----------------------------------------------+--------------+
    | Name                    | Release(s)                    | Stemcell(s)                                  | Cloud Config |
    +-------------------------+-------------------------------+----------------------------------------------+--------------+
    | cf-e82cbf44613594d8a155 | cf-autoscaling/28             | bosh-aws-xen-hvm-ubuntu-trusty-go_agent/3140 | none         |
    |                         | cf-mysql/23                   |                                              |              |
    |                         | cf/225                        |                                              |              |
    |                         | diego/0.1441.0                |                                              |              |
    |                         | etcd/18                       |                                              |              |
    |                         | garden-linux/0.327.0          |                                              |              |
    |                         | notifications-ui/10           |                                              |              |
    |                         | notifications/19              |                                              |              |
    |                         | push-apps-manager-release/397 |                                              |              |
    +-------------------------+-------------------------------+----------------------------------------------+--------------+
    ```

1. Download the manifest.

    ```
    $ bosh download manifest cf-64f27267ea9cbfe823e1 /tmp/cf.yml
    Acting as user 'director' on deployment 'cf-e82cbf44613594d8a155' on 'p-bosh-30c19bdd43c55c627d70'
    Deployment manifest saved to `/tmp/cf.yml'
    ```

1. Set BOSH to use that deployment.

    `bosh deployment /tmp/cf.yml`

1. Check for place-holder VMs.

    ```
    $ bosh vms
    Acting as user 'director' on 'p-bosh-30c19bdd43c55c627d70'
    Deployment `cf-e82cbf44613594d8a155'

    Director task 33

    Task 33 done

    +----------------------------------------------------------------+--------------------+--------------------------------------------------------------+------------+
    | Job/index                                                      | State              | Resource Pool                                                | IPs        |
    +----------------------------------------------------------------+--------------------+--------------------------------------------------------------+------------+
    | unknown/unknown                                                | unresponsive agent |                                                              |            |
    | unknown/unknown                                                | unresponsive agent |                                                              |            |
    | unknown/unknown                                                | unresponsive agent |                                                              |            |
    ```
   <p class="note"><strong>Note</strong>: If you do not see these three `unknown/unknown` items on your list, instead you might see the mysql-partition VMs. If so, that's good, it means that your VMs were not destroyed, merely powered off or otherwise. <strong>Disregard the rest of the instructions in Recovering MySQL and skip to Part B: Bootstrapping.</strong>
   </p>

1. Run BOSH's cloud check to clear out the place-holders. Select the option to **Delete VM reference** if given the option.

    ```
    $ bosh cck
    Acting as user 'director' on deployment 'cf-e82cbf44613594d8a155' on 'p-bosh-30c19bdd43c55c627d70'
    Performing cloud check...

    Director task 34
      Started scanning 22 vms
      Started scanning 22 vms > Checking VM states. Done (00:00:10)
      Started scanning 22 vms > 19 OK, 0 unresponsive, 3 missing, 0 unbound, 0 out of sync. Done (00:00:00)
         Done scanning 22 vms (00:00:10)

      Started scanning 10 persistent disks
      Started scanning 10 persistent disks > Looking for inactive disks. Done (00:00:02)
      Started scanning 10 persistent disks > 10 OK, 0 missing, 0 inactive, 0 mount-info mismatch. Done (00:00:00)
         Done scanning 10 persistent disks (00:00:02)

    Task 34 done

    Started		2015-11-26 01:42:42 UTC
    Finished	2015-11-26 01:42:54 UTC
    Duration	00:00:12

    Scan is complete, checking if any problems found...

    Found 3 problems

    Problem 1 of 3: VM with cloud ID `i-afe2801f' missing.
      1. Skip for now
      2. Recreate VM
      3. Delete VM reference
    Please choose a resolution [1 - 3]: 3

    Problem 2 of 3: VM with cloud ID `i-36741a86' missing.
      1. Skip for now
      2. Recreate VM
      3. Delete VM reference
    Please choose a resolution [1 - 3]: 3

    Problem 3 of 3: VM with cloud ID `i-ce751b7e' missing.
      1. Skip for now
      2. Recreate VM
      3. Delete VM reference
    Please choose a resolution [1 - 3]: 3

    Below is the list of resolutions you've provided
    Please make sure everything is fine and confirm your changes

      1. VM with cloud ID `i-afe2801f' missing.
         Delete VM reference

      2. VM with cloud ID `i-36741a86' missing.
         Delete VM reference

      3. VM with cloud ID `i-ce751b7e' missing.
         Delete VM reference

    Apply resolutions? (type 'yes' to continue): yes
    Applying resolutions...

    Director task 35
      Started applying problem resolutions
      Started applying problem resolutions > missing_vm 11: Delete VM reference. Done (00:00:00)
      Started applying problem resolutions > missing_vm 27: Delete VM reference. Done (00:00:00)
      Started applying problem resolutions > missing_vm 26: Delete VM reference. Done (00:00:00)
         Done applying problem resolutions (00:00:00)

    Task 35 done

    Started		2015-11-26 01:47:08 UTC
    Finished	2015-11-26 01:47:08 UTC
    Duration	00:00:00
    Cloudcheck is finished
    ```
1. Modify the deployment.

    `bosh edit deployment`
    
    This will launch a `vi` editor. Here are the exact steps you'll need to do inside of `vi`:
    - Search for the jobs section: `/^jobs`
    - Search for the mysql-partition: `/name: mysql-partition`
    - Search for the update section: `/update`
    - Update max_in_flight to `3`.
    - Add a line: `canaries: 0` below the `max_in_flight` line.

1. Apply these changes by running `bosh deploy`.

    **BE SURE** to stop and look over the changes carefully. Type `yes` if and only if this is the only change:
  
    ```
    Jobs
    mysql-partition-f8abb6ac43ccc9fe16d7
      update
        Â± max_in_flight:
          - 1
          + 3
        + canaries: 0
        
    Properties
    No changes

    Please review all changes carefully

    Deploying
    ---------
    Are you sure you want to deploy? (type 'yes' to continue):
    ```
    You will see BOSH re-creating the lost VMs:
    
    ```
    Done creating bound missing vms > mysql-partition-f8abb6ac43ccc9fe16d7/0 (00:01:54)
    Done creating bound missing vms > mysql-partition-f8abb6ac43ccc9fe16d7/2 (00:01:54)
    Done creating bound missing vms > mysql-partition-f8abb6ac43ccc9fe16d7/1 (00:02:04)
    ```
    Ultimately, you will see this:
    
    ```
       Failed updating job mysql-partition-f8abb6ac43ccc9fe16d7 > mysql-partition-f8abb6ac43ccc9fe16d7/1: `mysql-partition-f8abb6ac43ccc9fe16d7/1' is not running after update (00:05:57)
       Failed updating job mysql-partition-f8abb6ac43ccc9fe16d7 > mysql-partition-f8abb6ac43ccc9fe16d7/0: `mysql-partition-f8abb6ac43ccc9fe16d7/0' is not running after update (00:05:59)
       Failed updating job mysql-partition-f8abb6ac43ccc9fe16d7 > mysql-partition-f8abb6ac43ccc9fe16d7/2: `mysql-partition-f8abb6ac43ccc9fe16d7/2' is not running after update (00:06:01)
       Failed updating job mysql-partition-f8abb6ac43ccc9fe16d7 (00:06:01)

    Error 400007: `mysql-partition-f8abb6ac43ccc9fe16d7/1' is not running after update
    ````
    **This is OK**. Now you can proceed to the standard bootstrap process. You can run `bosh vms` a second time to validate that you now have three VMs all in the failing state:
    
    ```
    | mysql-partition-f8abb6ac43ccc9fe16d7/0                         | failing  | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.12 |
    | mysql-partition-f8abb6ac43ccc9fe16d7/1                         | failing  | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.60 |
    | mysql-partition-f8abb6ac43ccc9fe16d7/2                         | failing  | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.61 |
    ```

----

## <a id='export'>Part B: Bootstrapping </a> ##

- Follow the [Bootstrap](https://docs.pivotal.io/p-mysql/bootstrapping.html) process per normal.

----

After completing the bootstrap process, you'll be rewarded with a healthy, stable MySQL cluster again:

```
| mysql-partition-f8abb6ac43ccc9fe16d7/0                         | running | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.12 |
| mysql-partition-f8abb6ac43ccc9fe16d7/1                         | running | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.60 |
| mysql-partition-f8abb6ac43ccc9fe16d7/2                         | running | mysql-partition-f8abb6ac43ccc9fe16d7                         | 10.0.16.61 |
```


## <a id='export'>Common Issues </a> ##

**If you experience an error that looks something like:**

> Received disconnect from 10.0.1.19: 2: Too many authentication failures for bosh_64898ue98

You likely have too many key identities loaded in your authentication agent. You can clear all of them using `ssh-add -D`.


- If you have an issue with not being able to `monit start` it may be due to a bug in mariadb_ctrl that doesn't collect a defunct process. In that case, you'll need to call support in order to get monit un-stuck. Please also see the Known Issues file.

- FIXME: document the use of `bosh instances --ps`

```
+----------------------------------------------------------------+---------+--------------------------------------------------------------+------------+
| mysql-partition-672bf7d9fa6b1a201c94/0                         | running | mysql-partition-672bf7d9fa6b1a201c94                         | 10.0.16.12 |
|   mariadb_ctrl                                                 | running |                                                              |            |
|   galera-healthcheck                                           | running |                                                              |            |
|   gra-log-purger-executable                                    | running |                                                              |            |
+----------------------------------------------------------------+---------+--------------------------------------------------------------+------------+
| mysql-partition-672bf7d9fa6b1a201c94/1                         | stopped | mysql-partition-672bf7d9fa6b1a201c94                         | 10.0.16.60 |
+----------------------------------------------------------------+---------+--------------------------------------------------------------+------------+
| mysql-partition-672bf7d9fa6b1a201c94/2                         | stopped | mysql-partition-672bf7d9fa6b1a201c94                         | 10.0.16.61 |
+----------------------------------------------------------------+---------+--------------------------------------------------------------+------------+
```
